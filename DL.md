# Deep Learning

### [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/)

### Practical Deep Learning [Link](http://course.fast.ai/lessons/lessons.html)

### Unsupervised Pre-training, Fine-tuning[Link](https://www.youtube.com/watch?v=Oq38pINmddk)

### Backpropogation and Initializations [Link](https://docs.google.com/document/d/e/2PACX-1vQ8kpJtcMpsr0ePhB78OngfjI14TMSvUJJ1m_RtmgLRNqUsWlURXZoQNtYg2TrkOlNMmxS9HNE9e1WB/pub)

### Softmax CrossEntropy Backprop [Link](https://www.ics.uci.edu/~pjsadows/notes.pdf)

### Gradient Descent and Optimization [Link1](https://docs.google.com/document/d/e/2PACX-1vSRkLjI5Kpt8dPyN5wylb1ZgkdhzKTv21MrRIWktbOymwHzHOLXzxer4K57jnVmSa5kybLieV8Lc4CF/pub) [Link2](https://docs.google.com/document/d/e/2PACX-1vQmomWlyGsNQUyvBHRK6YMhiLJ6ee0PWPG4hZZyLRFHFE412lZgO5qHZ7iUkxltM0rxhJ8uf79bZSSk/pub)

### Momentum [Link](https://distill.pub/2017/momentum/)

### Dropouts [Link](https://medium.com/@bingobee01/a-review-of-dropout-as-applied-to-rnns-72e79ecd5b7b)

### Convolutional Neural Networks [Link](https://docs.google.com/document/d/e/2PACX-1vRG_-7Xe6DTwg-yfwPmYMoezS8WDYpWjC7jTnQeJnA4dDAiXlLBHwgkzQl_j-fCpZQTmuYU99ePGXww/pub)

### Attention [Link](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

### Structuring Your Tensorflow Models [Link](https://danijar.com/structuring-your-tensorflow-models/)

### AUTODIFF [Link](http://videolectures.net/deeplearning2017_johnson_automatic_differentiation/)

### JAX [Link](https://colinraffel.com/blog/you-don-t-know-jax.html)

### Vanishing Gradients [Link](https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/) [Link2](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484)

#### Batch Normalization [Link](https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739)



