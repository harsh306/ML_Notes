# Deep Learning Basics [Quick reads]

#### Prerequisites 
- Linear Algebra and Calculus
- Intro to probability
- Entropy and information
- Loss Functions
- Maximum Likelihood Estimate


#### Gradient Descent and Optimization [Link1](https://docs.google.com/document/d/e/2PACX-1vSRkLjI5Kpt8dPyN5wylb1ZgkdhzKTv21MrRIWktbOymwHzHOLXzxer4K57jnVmSa5kybLieV8Lc4CF/pub) [Link2](https://docs.google.com/document/d/e/2PACX-1vQmomWlyGsNQUyvBHRK6YMhiLJ6ee0PWPG4hZZyLRFHFE412lZgO5qHZ7iUkxltM0rxhJ8uf79bZSSk/pub)
- Taylor Series Approximation [Link](https://suzyahyah.github.io/calculus/optimization/2018/04/06/Taylor-Series-Newtons-Method.html)
- Momentum [Link](https://distill.pub/2017/momentum/)

#### Backpropogation and Initializations 
- Xavier/Glorot [Link](https://www.deeplearning.ai/ai-notes/initialization/)
- He init [Link](https://medium.com/@shoray.goel/kaiming-he-initialization-a8d9ed0b5899)
- Softmax CrossEntropy Backprop [Link](https://www.ics.uci.edu/~pjsadows/notes.pdf)
- Activations and differentiations


#### Unsupervised Pre-training, Fine-tuning[Link](https://www.youtube.com/watch?v=Oq38pINmddk)

#### Types of Network:
- Convolutional Neural Networks [Link](https://docs.google.com/document/d/e/2PACX-1vRG_-7Xe6DTwg-yfwPmYMoezS8WDYpWjC7jTnQeJnA4dDAiXlLBHwgkzQl_j-fCpZQTmuYU99ePGXww/pub)
- LSTMs and RNNs
- GANs [Link](https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html)
- Transformer and attention [Link](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

#### Regularizations 
- Dropouts [Link](https://medium.com/@bingobee01/a-review-of-dropout-as-applied-to-rnns-72e79ecd5b7b)
- L1
- L2


#### Common Problems and Tricks
- Vanishing Gradients [Link](https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/) [Link2](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484)
- Exploding Gradients [Link](https://www.machinecurve.com/index.php/2019/09/16/he-xavier-initialization-activation-functions-choose-wisely/)
- Data Augment [Stanford](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks) 
- Best practice [Andrej](http://karpathy.github.io/2019/04/25/recipe/)
- Batch Normalization [Link](https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739)
- Skip Connections
- Modal Collapse in GANs [Link]()





# Deep Learning General 

#### [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/)

#### Practical Deep Learning [Link](http://course.fast.ai/lessons/lessons.html)

#### Deep Learning Optimization [Link](https://github.com/harsh306/awesome-nn-optimization)

#### Structuring Your Tensorflow Models [Link](https://danijar.com/structuring-your-tensorflow-models/)

#### AUTODIFF [Link](http://videolectures.net/deeplearning2017_johnson_automatic_differentiation/)  [JAX](https://colinraffel.com/blog/you-don-t-know-jax.html)





