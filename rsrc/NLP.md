# NLP 

### Basics
- TF-IDF [Link](http://www.tfidf.com)
- BOW and N-gram [Link](https://machinelearningmastery.com/gentle-introduction-bag-words-model/) 
- NLP pre-processing [Link](https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79)
- Word2Vec [Link](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/); Negative smapling [Link](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/) [Link2](https://www.coursera.org/learn/nlp-sequence-models/lecture/Iwx0e/negative-sampling)
  - As Siamese [Link](https://www.youtube.com/watch?v=f7o8aDNxf7k)   
- Sequence Models [Link](https://docs.google.com/document/d/1F3ldWUp7zy0xmVbWS9RLSeCf_SNuppzq4Bd_9AjkI8o/edit?usp=sharing) [Link2](https://distill.pub/2019/memorization-in-rnns/)

### Attention and its Advances
- Attention and Transformer [Link](http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/) [Link2](https://www.youtube.com/watch?v=4Bdc55j80l8) [Link3](https://www.youtube.com/watch?v=XSSTuhyAmnI)
- Bert vs Transformer [Link](https://ai.stackexchange.com/questions/23221/how-is-bert-different-from-the-original-transformer-architecture)
- GPT vs Transformer [Link](https://ai.stackexchange.com/questions/27038/why-does-gpt-2-exclude-the-transformer-encoder?rq=1)
- Genralized Language Models: CoVe, ELMO, GPT and BERT [Link](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html) [Link2](http://jalammar.github.io/illustrated-bert/) 

### Named Entity Recognition
- Part of speech tagging [Link](https://www.youtube.com/watch?v=fv6Z3ZrAWuU)
- Hidden Markov Model [Link](https://www.youtube.com/watch?v=fX5bYmnHqqE)
- Veterbi algorithm [Link](https://www.youtube.com/watch?v=IqXdjdOgXPM)

### Topic Modeling
- LDA Topic Modelling [Link](https://www.youtube.com/watch?v=3mHy4OSyRf0) [Link-Code](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24) 
- - Cornell Topic Model [Link](https://mimno.infosci.cornell.edu/papers/2017_fntir_tm_applications.pdf)
- LSH +  (Any embedding Method such as Bert, word2vec, tf-idf + K-means



### [Georgia Tech](https://github.com/jacobeisenstein/gt-nlp-class/tree/master/notes)

### Regex [Link](https://regexone.com/references/python) [Link2](https://www.tutorialspoint.com/python/python_reg_expressions.htm)

### Glove vs Word2vec [Link](https://www.quora.com/How-is-GloVe-different-from-word2vec)

### Attention [Link](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) [Link2](https://distill.pub/2016/augmented-rnns/)

[More Bert](http://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/)
[More ELMO](http://mlexplained.com/2018/06/15/paper-dissected-deep-contextualized-word-representations-explained/)


