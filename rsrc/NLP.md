# NLP 

### [Georgia Tech](https://github.com/jacobeisenstein/gt-nlp-class/tree/master/notes)

### Regex [Link](https://regexone.com/references/python) [Link2](https://www.tutorialspoint.com/python/python_reg_expressions.htm)

### BOW and N-gram [Link](https://machinelearningmastery.com/gentle-introduction-bag-words-model/) 

### TF-IDF [Link](http://www.tfidf.com)

### NLP pre-processing [Link](https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79)

### LDA Topic Modelling [Link](https://www.youtube.com/watch?v=3mHy4OSyRf0) [Link-Code](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24) 

### Word2Vec [Link](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) 

### Negative smapling [Link](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/) [Link2](https://www.coursera.org/learn/nlp-sequence-models/lecture/Iwx0e/negative-sampling)

### Glove vs Word2vec [Link](https://www.quora.com/How-is-GloVe-different-from-word2vec)

### Sequence Models [Link](https://docs.google.com/document/d/1F3ldWUp7zy0xmVbWS9RLSeCf_SNuppzq4Bd_9AjkI8o/edit?usp=sharing) [Link2](https://distill.pub/2019/memorization-in-rnns/)

### Attention [Link](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) [Link2](https://distill.pub/2016/augmented-rnns/)

### Genralized Language Models: CoVe, ELMO, GPT and BERT [Link](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html) [Link2](http://jalammar.github.io/illustrated-bert/) 

[More Bert](http://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/)
[More ELMO](http://mlexplained.com/2018/06/15/paper-dissected-deep-contextualized-word-representations-explained/)

[Transformer](http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/)
